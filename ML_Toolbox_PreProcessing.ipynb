{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ace16063",
   "metadata": {},
   "source": [
    "# Master Machine Learning Preprocessing Template\n",
    "This notebook contains all the core techniques learned in Days 1-30."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "99bdfb62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Toolkit Ready!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, OneHotEncoder, OrdinalEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "print(\"Toolkit Ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1327c337",
   "metadata": {},
   "source": [
    "### 1. The Data Check-up\n",
    "Before applying any preprocessing, you must understand your data's health.\n",
    "*   **Missing Values:** Identify columns with null entries (`NaN`). If not handled, most models will crash.\n",
    "*   **Duplicates:** Repeated rows can bias your model.\n",
    "*   **Data Types:** Ensure numbers are `int/float` and categories are `object`.\n",
    "*   **Skewness:** Check if the data distribution is Gaussian (Bell Curve) or skewed.\n",
    "\n",
    "Use the commands below to diagnose your dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5604cae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.read_csv('your_file.csv')\n",
    "\n",
    "# 1. Structure & Integrity\n",
    "# df.info() # Check data types & non-null counts\n",
    "# print(f\"Duplicate rows: {df.duplicated().sum()}\") # Check duplicates\n",
    "\n",
    "# 2. Missing Values Analysis\n",
    "# print(df.isnull().mean() * 100) # Percentage of missing data\n",
    "\n",
    "# 3. Numerical Distribution (Check for skewness/outliers)\n",
    "# print(df.describe())\n",
    "\n",
    "# 4. Categorical Distribution (Check for cardinality/rare labels)\n",
    "# print(df['category_col'].value_counts())\n",
    "\n",
    "# 5. Correlation Check\n",
    "# print(df.corr()['target_col'].sort_values()) # Correlation with target"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1dab6bb",
   "metadata": {},
   "source": [
    "### 2. Imputation Strategies (Handling Missing Data)\n",
    "Imputation is the process of filling missing values with statistical estimates.\n",
    "\n",
    "**Strategies for Numerical Data:**\n",
    "1.  **Mean:** Use when data is normally distributed (no outliers).\n",
    "2.  **Median:** Use when data is skewed (robust to outliers).\n",
    "3.  **Arbitrary:** Replacing with -1 or 99 (use if data is not missing at random).\n",
    "\n",
    "**Strategies for Categorical Data:**\n",
    "1.  **Most Frequent (Mode):** Replaces missing values with the most common category.\n",
    "2.  **Constant:** Fills missing values with a new category like \"Missing\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d848187e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Numerical Imputation\n",
    "si_mean = SimpleImputer(strategy='mean')     # Use for Normal distribution\n",
    "si_median = SimpleImputer(strategy='median') # Use for Skewed data (Example: Age, Salary)\n",
    "si_arb = SimpleImputer(strategy='constant', fill_value=-1) # Flag missing values explicitly\n",
    "\n",
    "# 2. Categorical Imputation\n",
    "si_mode = SimpleImputer(strategy='most_frequent') # Replace with Mode (safe default)\n",
    "si_miss = SimpleImputer(strategy='constant', fill_value='Missing') # Create specific \"Missing\" category\n",
    "\n",
    "# 3. Advanced: KNN Imputation (Multivariate)\n",
    "# Uses other features to guess the missing value by finding 'neighbors'\n",
    "from sklearn.impute import KNNImputer\n",
    "knn_imputer = KNNImputer(n_neighbors=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f404a99e",
   "metadata": {},
   "source": [
    "### 3. Encoding (Converting Words to Numbers)\n",
    "Machine Learning models only understand numbers. Encoding converts categorical text into numerical format.\n",
    "\n",
    "**Key Techniques:**\n",
    "1.  **One-Hot Encoding (Nominal Data):**\n",
    "    *   **What it is:** Creates a new binary column for each category (e.g., Color -> Red, Green, Blue).\n",
    "    *   **When to use:** When categories have **no inherent order** (Gender, City, Brand).\n",
    "    *   **Note:** Use `drop='first'` to avoid the \"Dummy Variable Trap\" (multicollinearity).\n",
    "\n",
    "2.  **Ordinal Encoding (Ordinal Data):**\n",
    "    *   **What it is:** Assigns an integer rank to each category (e.g., Low=0, Medium=1, High=2).\n",
    "    *   **When to use:** When categories have a clear **rank or order** (Education Level, Satisfaction Rating)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d75bc7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. One-Hot Encoding (Nominal: Gender, City)\n",
    "# drop='first': Removes 1 dummy col to prevent multicollinearity (dummy variable trap)\n",
    "# handle_unknown='ignore': Handles new categories in test data gracefully (all zeros)\n",
    "ohe = OneHotEncoder(drop='first', sparse_output=False, handle_unknown='ignore')\n",
    "\n",
    "# 2. Ordinal Encoding (Ordinal: Education, Satisfaction)\n",
    "# Define order manually: [Col_1_Order, Col_2_Order]\n",
    "oe = OrdinalEncoder(categories=[\n",
    "    ['School', 'UG', 'PG'],                # e.g., Education\n",
    "    ['Low', 'Medium', 'High']              # e.g., Salary Grade\n",
    "])\n",
    "\n",
    "# 3. Label Encoding (Target Variable ONLY)\n",
    "# Encodes y (Target) into 0, 1, 2... Do NOT use for X (features)\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "le = LabelEncoder()\n",
    "# y_train = le.fit_transform(y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07f78b61",
   "metadata": {},
   "source": [
    "### 4. Scaling (Feature Scaling)\n",
    "Scaling ensures that all features contribute equally to the result by bringing them to a similar range. Without scaling, a feature like \"Salary\" (range 20k-100k) will dominate \"Age\" (range 18-90).\n",
    "\n",
    "**Key Scalers:**\n",
    "1.  **StandardScaler (Z-Score Normalization):**\n",
    "    *   **How it works:** Centers data around 0 with a standard deviation of 1. Formula: $z = \\frac{x - \\mu}{\\sigma}$\n",
    "    *   **When to use:** Default choice for most algorithms (Logistic Regression, SVM, KNN). Preserves outliers.\n",
    "\n",
    "2.  **MinMaxScaler (Normalization):**\n",
    "    *   **How it works:** Squeezes data between 0 and 1. Formula: $x' = \\frac{x - min}{max - min}$\n",
    "    *   **When to use:** Deep Learning (CNNs, ANNs) or when you know the distribution is not Gaussian. Sensitive to outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4287f905",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. StandardScaler (Z-Score)\n",
    "# Mean=0, Std=1. Does NOT handle outliers (they stay outliers).\n",
    "# Use for: Linear Regression, Logistic Regression, KNN, SVM, PCA\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# 2. MinMaxScaler\n",
    "# Range [0, 1]. Compress data. Sensitive to outliers.\n",
    "# Use for: Neural Networks (CNN/ANN), Algorithms using distances\n",
    "minmax = MinMaxScaler()\n",
    "\n",
    "# 3. RobustScaler\n",
    "# Scaled using Median and IQR (Interquartile Range).\n",
    "# Use for: Data with heavy outliers\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "robust = RobustScaler()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dfd6b65",
   "metadata": {},
   "source": [
    "### 5. The Ultimate Workflow (ColumnTransformer + Pipeline)\n",
    "Instead of applying steps manually one by one, we bundle them.\n",
    "\n",
    "*   **Pipeline:** Chains sequential steps together (e.g., Impute -> Scale -> Model).\n",
    "*   **ColumnTransformer:** Applies different transformations to different columns **in parallel** (e.g., Scale numerical columns vs Encode categorical columns).\n",
    "\n",
    "**Why use this?**\n",
    "1.  **Prevents Data Leakage:** Ensures statistics (mean, variance) are calculated only on `X_train` and applied to `X_test`.\n",
    "2.  **Production Ready:** You can save the entire object as a `.pkl` file and deploy it easily."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "721d8d81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Define which columns get which treatment\n",
    "num_cols = ['age', 'fare'] # example columns\n",
    "cat_cols = ['embarked', 'sex'] # example columns\n",
    "\n",
    "# Step 2: Create sub-pipelines\n",
    "num_pipe = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "cat_pipe = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "    ('encoder', OneHotEncoder(drop='first', handle_unknown='ignore'))\n",
    "])\n",
    "\n",
    "# Step 3: Combine into ColumnTransformer\n",
    "preprocessor = ColumnTransformer(transformers=[\n",
    "    ('num_section', num_pipe, num_cols),\n",
    "    ('cat_section', cat_pipe, cat_cols)\n",
    "])\n",
    "\n",
    "# Now you can just use: \n",
    "# X_train_transformed = preprocessor.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36cadd9b",
   "metadata": {},
   "source": [
    "### 6. Alternative Workflow (Sequential Pipeline)\n",
    "This method stacks `ColumnTransformers` sequentially inside a `Pipeline`.\n",
    "\n",
    "**Flow:**\n",
    "Step 1 (Impute) $\\rightarrow$ Output $\\rightarrow$ Step 2 (Encode) $\\rightarrow$ Output $\\rightarrow$ Step 3 (Scale)\n",
    "\n",
    "**Critical Warning:**\n",
    "In this approach, the output of Step 1 is a NumPy array without column names. If a step (like OneHotEncoding) adds new columns, the indices of your columns will shift. You must manually calculate the new column index for the subsequent steps.\n",
    "\n",
    "**When to use:**\n",
    "Use this only if you need the output of one transformer (e.g., filled missing values) before the next transformer can work (e.g., feature extraction). Otherwise, use the Method #5 (Parallel) approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17eb9a46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Impute Missing Values (ColumnTransformer 1)\n",
    "trnf1 = ColumnTransformer(transformers=[\n",
    "    ('impute_age', SimpleImputer(), [2]), # Input Column Index\n",
    "    ('impute_embarked', SimpleImputer(strategy='most_frequent'), [6])\n",
    "], remainder='passthrough')\n",
    "\n",
    "# Step 2: Encoding (ColumnTransformer 2)\n",
    "# Note: You must know the new column indices after Step 1\n",
    "trnf2 = ColumnTransformer(transformers=[\n",
    "    ('ohe_sex_embarked', OneHotEncoder(sparse_output=False, handle_unknown='ignore'), [1, 6])\n",
    "], remainder='passthrough')\n",
    "\n",
    "# Step 3: Scaling (ColumnTransformer 3)\n",
    "trnf3 = ColumnTransformer(transformers=[\n",
    "    ('scale', MinMaxScaler(), slice(0, 10)) # Scaling all columns\n",
    "])\n",
    "\n",
    "# Step 4: Create the Sequential Pipeline\n",
    "pipe = Pipeline([\n",
    "    ('step1', trnf1),\n",
    "    ('step2', trnf2),\n",
    "    ('step3', trnf3),\n",
    "    # ('model', DecisionTreeClassifier())\n",
    "])\n",
    "\n",
    "# pipe.fit(X_train, y_train)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jupyter_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
